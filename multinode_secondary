master node ---
sudo adduser hduser
sudo visudo
hduser ALL=(ALL) NOPASSWD:ALL
sudo nano /etc/resolv.conf
nameserver 192.168.72.20
sudo apt update -y
sudo apt install ssh vim pdsh openjdk-8-jdk git -y
sudo mkdir /usr/local/hadoop
sudo apt install net-tools -y
sudo systemctl start ssh
sudo systemctl enable ssh
apache hadoop download frist binary hadoop-3-4-0.tar.gz
or
wget -c -O hadoop.tar.gz https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz
tar -xvzf hadoop-3.4.0.tar.gz
sudo mv hadoop-.3.4.0/* /usr/local/hadoop
la -all /usr/local/hadoop
sudo chown -R hduser:hduser /usr/local/hadoop
sudo chown -R 755 /usr/local/hadoop
nano .bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_MARPED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export CLASSPATH=$CLASSPATH:$(hadoop classpath)
export PDSH_RCMD_TYPE=ssh
source .bashrc
activity<-
change date
sudo init 0
creating clone
secondaryNN
DN1
DN2
master---
sudo hostnamectl set-hostname master
sudo hostaname master
cat ~/.bashrc
copy JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
sudo nano hadoop.env.sh
paste here
sudo nano core-site.xml
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
sudo nano hdfs-site.xml
<property>
        <name>dfs.name.dir</name>
        <value>/usr/local/hadoop/hd-data/nn</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>SNN:9869</value>
    </property>
sudo nano yarn-site.xml
 <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
sudo mapred-site.xml
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
nano worker
DN1
DN2
secondary---
ip a
dn1---
ip a
dn2---
ip a
master---
ip a
sudo nano /etc/hosts
192.168.144.129 master
192.168.144.140 SNN
192.168.144.130 DN1
192.168.144.131 DN2
frist ssh config then copy all files of master in secondary dn1 dn2
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@master
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@SNN
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@DN1
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@DN2
ssh hduser@master
cd /usr/local/hadoop
scp /etc/hosts hduser@SNN:/home/hduser
scp /etc/hosts hduser@DN1:/home/hduser
scp /etc/hosts hduser@DN2:/home/hduser
scp /usr/local/hadoop/etc/hadoop/core-site.xml hduser@SNN:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/core-site.xml hduser@DN1:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/core-site.xml hduser@DN2:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/hdfs-site.xml hduser@SNN:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/hdfs-site.xml hduser@DN1:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/hdfs-site.xml hduser@DN2:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/yarn-site.xml hduser@SNN:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/yarn-site.xml hduser@DN1:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/yarn-site.xml hduser@DN2:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/mapred-site.xml hduser@SNN:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/mapred-site.xml hduser@DN1:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/mapred-site.xml hduser@DN2:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/hadoop.env.sh hduser@SNN:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/hadoop.env.sh hduser@DN1:/usr/local/hadoop/etc/hadoop
scp /usr/local/hadoop/etc/hadoop/hadoop.env.sh hduser@DN2:/usr/local/hadoop/etc/hadoop
secondary----
nano hdfs-site-xml
<property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>/usr/local/hadoop/hd-data/snn</value>
    </property>
#--------------------
checkpoint duration by default 1hr
dfs.namenode.checkpoint.period
1800
#-------------------------------

dn1---
core-site.xml same
hdfs-site.xml
hdfs-site.xml
<property>
        <name>dfs.data.dir</name>
        <value>/usr/local/hadoop/hd-data/dn</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
mapred-site.xml----same
yarn-site.xml
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/data</value>
    </property>
    <property>
        <name>yarn.nodemanager.logs-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/logs</value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-perdisk-percentage</name>
        <value>99.9</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
 workers
 localhost
 copy to dn2 #and dn3
 scp hdfs-site.xml hduser@DN2:/usr/local/hadoop/etc/hadoop
 same for yarn site
 scp yarn-site.xml hduser@DN2:/usr/local/hadoop/etc/hadoop
 master----
 mkdir /usr/local/hadoop/hd-data
 dn1---
 mkdir /usr/loal/hadoop/hd-data
 mkdir /usr/local/hadoop/yarn
 same to dn2 #dn3
 master --
 hadoop namenode -format
 dn2--
sudo cp hosts /etc/hosts
master---
start-dfs.sh
start-yarn.sh
start-all.sh
dn1 -ping master
dn2-ping master
master---
ssh -t hduser@DN1
ssh -t hduser@DN2
------------------------
web---http://master:9870
------------------------
master   node =
make a dir
cd Desktop
mkdir /usr/local/hadoop/hd-data
datanode (DN1) ==================
mkdir /usr/local/hadoop/hd-data
mkdir /usr/local/hadoop/yarn
datanode (DN2) ==================
mkdir /usr/local/hadoop/hd-data
mkdir /usr/local/hadoop/yarn
master   node ==================
hadoop namenode -format
ls /usr/local/hadoop/hd-data/nn
ls /usr/local/hadoop/hd-data/nn/current/
start-dfs.sh
now try to ping each other
create a dir and file.txt and try to copy or move in master                 
login to the browser
http://master:9870
===========================================================
now add a new node datanode (DN3)
login to the master
 ping to each other
 start-all.sh
jps
as we have taken a clone just open a datanode3(DN3)
need to set the hostname
sudo hostnamectl set-hostname DN3
sudo hostname DN3
 exit
open new terminal foor apply the changes done
 ip a                                ......... 192.168.144.132 /24
master---------     
sudo nano /etc/hosts
192.168.144.132 DN3
nano /usr/local/hadoop/etc/hadoop/workers
edit this file and DN3
DN3
Datanode2---------     
jps
sudo nano /etc/hosts        
edit this file
192.168.144.132 DN3
scp /etc/hosts hduser@DN3:/home/hduser    .......  copying the hostfile from DN2  and pate it DN3 
if we want to copy same file that are too much at same dest use the below command 
rsync -a /usr/local/hadoop/etc/hadoop/* hduser@DN3:/usr/local/hadoop/etc/hadoop
Datanode3----------     
cd
cat /etc/hosts
cd
ls
sudo cp hosts /etc/hosts
cat /etc/hosts
ping master
hadoop-daemon.sh start datanode      ...(to start a individual machine use this command)
jps
Datanode1---------     
cd Desktop
jps
sudo nano /etc/hosts
edit this file 
192.168.144.132 DN3
master------------    
hdfs dfsadmin -refreshnodes              .......(and refersh in browser)
Datanode3----------    
cd Desktop
yarn-daemon.sh start manager
jps
master-----------    
now in master you can create a dir and file and can perform any opertaion



===============================================================================================================================

